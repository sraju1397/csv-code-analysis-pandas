{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-18T13:05:35.924228Z",
     "start_time": "2024-07-18T13:05:35.613661Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:05:35.927269Z",
     "start_time": "2024-07-18T13:05:35.925211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_date_from_filename(file_name):\n",
    "    # Extract date from filename\n",
    "    match = re.search(r'(\\d{2})(\\d{2})(\\d{4})\\.csv$', file_name)\n",
    "    if match:\n",
    "        day, month, year = match.groups()\n",
    "        return datetime(int(year), int(month), int(day))\n",
    "    return None\n"
   ],
   "id": "fc92f5892b015804",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:05:35.931487Z",
     "start_time": "2024-07-18T13:05:35.928037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_ticker(df_filter, file_name):\n",
    "    # Check if required columns exist\n",
    "    required_columns = ['Ticker', 'Time', 'Date']\n",
    "    missing_columns = [col for col in required_columns if col not in df_filter.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: Missing columns in {file_name}: {', '.join(missing_columns)}\")\n",
    "        return None\n",
    "\n",
    "    # Filter and process the dataframe\n",
    "    df_filter = df_filter[\n",
    "        df_filter['Ticker'].str.startswith('BANKNIFTY') &\n",
    "        ~df_filter['Ticker'].str.contains('CE', na=False) &\n",
    "        ~df_filter['Ticker'].str.contains('PE', na=False)\n",
    "    ]\n",
    "\n",
    "    # Group by Date and Time\n",
    "    grouped = df_filter.groupby(['Date', 'Time'])\n",
    "\n",
    "    # Process each group\n",
    "    result_rows = []\n",
    "    for _, group in grouped:\n",
    "        # Sum Volume and Open Interest\n",
    "        total_volume = group['Volume'].sum()\n",
    "        total_open_interest = group['Open Interest'].sum()\n",
    "\n",
    "        # Select the row with the highest Volume\n",
    "        max_volume_row = group.loc[group['Volume'].idxmax()].copy()\n",
    "\n",
    "        # Update Volume and Open Interest with the summed values\n",
    "        max_volume_row['Volume'] = total_volume\n",
    "        max_volume_row['Open Interest'] = total_open_interest\n",
    "\n",
    "        result_rows.append(max_volume_row)\n",
    "\n",
    "    # Combine results into a new DataFrame\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "\n",
    "    # Sort the result\n",
    "    result_df = result_df.sort_values(by=['Date', 'Time', 'Ticker'])\n",
    "\n",
    "    # Add filename column\n",
    "    # result_df['Filename'] = file_name\n",
    "\n",
    "    return result_df\n"
   ],
   "id": "58ae7a9c7310b2e5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:06:19.849272Z",
     "start_time": "2024-07-18T13:05:35.932849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main script\n",
    "path = 'data/'\n",
    "output_path = 'output'\n",
    "output_file = os.path.join(output_path, 'out.csv')\n",
    "processed_files_log = os.path.join(output_path, 'processed_files.txt')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Get the set of already processed files\n",
    "processed_files = set()\n",
    "if os.path.exists(processed_files_log):\n",
    "    with open(processed_files_log, 'r') as f:\n",
    "        processed_files = set(f.read().splitlines())\n",
    "\n",
    "# Collect all CSV files with their dates\n",
    "all_files = []\n",
    "for root, _, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            file_date = parse_date_from_filename(file)\n",
    "            if file_date:\n",
    "                all_files.append((os.path.join(root, file), file_date))\n",
    "\n",
    "# Sort files by date, newest first\n",
    "all_files.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Process files\n",
    "newly_processed_files = []\n",
    "for file_path, file_date in all_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    if filename in processed_files:\n",
    "        print(f\"Skipping {file_path} (already processed)\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        df_modified = filter_ticker(df, filename)\n",
    "\n",
    "        if df_modified is not None and not df_modified.empty:\n",
    "            # Append the modified data to the output CSV\n",
    "            df_modified.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False,\n",
    "                               encoding='utf-8')\n",
    "            print(f\"File processed: {file_path}\")\n",
    "            newly_processed_files.append(filename)\n",
    "        else:\n",
    "            print(f\"No data to write after filtering: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "\n",
    "# Update the processed files log\n",
    "with open(processed_files_log, 'a') as f:\n",
    "    for filename in newly_processed_files:\n",
    "        f.write(f\"{filename}\\n\")\n",
    "\n",
    "print(\"All files processed. Updated output file saved.\")\n"
   ],
   "id": "b4a785e414413db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processed: data/2013/August/NSEFO_30082013.csv\n",
      "File processed: data/2013/August/NSEFO_29082013.csv\n",
      "File processed: data/2013/August/NSEFO_26082013.csv\n",
      "File processed: data/2013/August/NSEFO_25082013.csv\n",
      "File processed: data/2013/August/NSEFO_24082013.csv\n",
      "File processed: data/2013/August/NSEFO_23082013.csv\n",
      "File processed: data/2013/August/NSEFO_22082013.csv\n",
      "File processed: data/2013/August/NSEFO_19082013.csv\n",
      "File processed: data/2013/August/NSEFO_18082013.csv\n",
      "File processed: data/2013/August/NSEFO_17082013.csv\n",
      "File processed: data/2013/August/NSEFO_16082013.csv\n",
      "File processed: data/2013/August/NSEFO_12082013.csv\n",
      "File processed: data/2013/August/NSEFO_11082013.csv\n",
      "File processed: data/2013/August/NSEFO_10082013.csv\n",
      "File processed: data/2013/August/NSEFO_09082013.csv\n",
      "File processed: data/2013/August/NSEFO_08082013.csv\n",
      "File processed: data/2013/August/NSEFO_05082013.csv\n",
      "File processed: data/2013/August/NSEFO_04082013.csv\n",
      "File processed: data/2013/August/NSEFO_03082013.csv\n",
      "File processed: data/2013/August/NSEFO_01082013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_15072013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_12072013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_11072013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_10072013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_09072013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_08072013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_05072013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_04072013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_03072013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_02072013.csv\n",
      "File processed: data/2013/July/GFDLNFO_BACKADJUSTED_01072013.csv\n",
      "File processed: data/2012/August/NSEFO_30082012.csv\n",
      "File processed: data/2012/August/NSEFO_29082012.csv\n",
      "File processed: data/2012/August/NSEFO_26082012.csv\n",
      "File processed: data/2012/August/NSEFO_25082012.csv\n",
      "File processed: data/2012/August/NSEFO_24082012.csv\n",
      "File processed: data/2012/August/NSEFO_23082012.csv\n",
      "File processed: data/2012/August/NSEFO_22082012.csv\n",
      "File processed: data/2012/August/NSEFO_19082012.csv\n",
      "File processed: data/2012/August/NSEFO_18082012.csv\n",
      "File processed: data/2012/August/NSEFO_17082012.csv\n",
      "File processed: data/2012/August/NSEFO_16082012.csv\n",
      "File processed: data/2012/August/NSEFO_12082012.csv\n",
      "File processed: data/2012/August/NSEFO_11082012.csv\n",
      "File processed: data/2012/August/NSEFO_10082012.csv\n",
      "File processed: data/2012/August/NSEFO_09082012.csv\n",
      "File processed: data/2012/August/NSEFO_08082012.csv\n",
      "File processed: data/2012/August/NSEFO_05082012.csv\n",
      "File processed: data/2012/August/NSEFO_04082012.csv\n",
      "File processed: data/2012/August/NSEFO_03082012.csv\n",
      "File processed: data/2012/August/NSEFO_01082012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_15072012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_12072012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_11072012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_10072012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_09072012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_08072012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_05072012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_04072012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_03072012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_02072012.csv\n",
      "File processed: data/2012/July/GFDLNFO_BACKADJUSTED_01072012.csv\n",
      "File processed: data/2011/August/NSEFO_30082011.csv\n",
      "File processed: data/2011/August/NSEFO_29082011.csv\n",
      "File processed: data/2011/August/NSEFO_26082011.csv\n",
      "File processed: data/2011/August/NSEFO_25082011.csv\n",
      "File processed: data/2011/August/NSEFO_24082011.csv\n",
      "File processed: data/2011/August/NSEFO_23082011.csv\n",
      "File processed: data/2011/August/NSEFO_22082011.csv\n",
      "File processed: data/2011/August/NSEFO_19082011.csv\n",
      "File processed: data/2011/August/NSEFO_18082011.csv\n",
      "File processed: data/2011/August/NSEFO_17082011.csv\n",
      "File processed: data/2011/August/NSEFO_16082011.csv\n",
      "File processed: data/2011/August/NSEFO_12082011.csv\n",
      "File processed: data/2011/August/NSEFO_11082011.csv\n",
      "File processed: data/2011/August/NSEFO_10082011.csv\n",
      "File processed: data/2011/August/NSEFO_09082011.csv\n",
      "File processed: data/2011/August/NSEFO_08082011.csv\n",
      "File processed: data/2011/August/NSEFO_05082011.csv\n",
      "File processed: data/2011/August/NSEFO_04082011.csv\n",
      "File processed: data/2011/August/NSEFO_03082011.csv\n",
      "File processed: data/2011/August/NSEFO_01082011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_15072011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_12072011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_11072011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_10072011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_09072011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_08072011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_05072011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_04072011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_03072011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_02072011.csv\n",
      "File processed: data/2011/July/GFDLNFO_BACKADJUSTED_01072011.csv\n",
      "All files processed. Updated output file saved.\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
